# How to Add New Documentation Sources to Archon

## Overview

This guide explains how to add new documentation sources to Archon's crawling and knowledge retrieval system.

## Process Summary

1. **Copy and Adapt the Crawler File**:
   - Create a new crawl file based on an existing one
   - Change URL sources and metadata

2. **Update Database Handling**:
   - Ensure proper source identification
   - Add clear record functions
   - Implement parallel processing

3. **Integrate with UI**:
   - Add buttons/options to the documentation page
   - Update functions to handle the new source

4. **Update Agent Integration**:
   - Modify the agent to recognize and use the new source

## Detailed Steps

### 1. Creating the Crawler File

1. **Copy an existing crawler file** like `crawl_pydantic_ai_docs.py` to create `crawl_your_source_docs.py`.

2. **Modify the source identifier**:
   ```python
   metadata = {
       "source": "your_source_docs",  # Change this unique identifier
       "chunk_size": len(chunk),
       "crawled_at": datetime.now(timezone.utc).isoformat(),
       "url_path": urlparse(url).path
   }
   ```

3. **Update the URL retrieval function**:
   ```python
   def get_your_source_docs_urls() -> List[str]:
       """Get URLs from your source sitemap or page."""
       sitemap_url = "https://docs.yoursource.com/sitemap.xml"
       # Implementation logic
       return urls
   ```

4. **Update clear_existing_records**:
   ```python
   def clear_existing_records():
       """Clear all existing records with source='your_source_docs' from the site_pages table."""
       try:
           result = supabase.table("site_pages").delete().eq("metadata->>source", "your_source_docs").execute()
           print("Cleared existing your_source_docs records from site_pages")
           return result
       except Exception as e:
           print(f"Error clearing existing records: {e}")
           return None
   ```

### 2. Database Integration

1. **Source Identification**:
   - Use a consistent source identifier in metadata
   - This allows filtering content by source

2. **Error Handling**:
   - Add detailed logging for errors
   - Implement tracebacks for easier debugging
   - Check connection status before operations

3. **Parallel Processing**:
   - Use `asyncio.gather` for parallel operations
   - Keep track of successful vs. failed operations
   - Implement concurrency controls for better performance

### 3. UI Integration

1. **Update streamlit_pages/documentation.py**:
   - Add buttons/sections for your new source
   - Add functions to handle the new source crawling

2. **Example UI Component**:
   ```python
   if st.button("Crawl Your Source Documentation", key="crawl_your_source"):
       with st.spinner("Crawling Your Source documentation..."):
           # Start the crawl process
           from archon.crawl_your_source_docs import start_crawl_with_requests
           tracker = start_crawl_with_requests(update_progress)
           st.session_state.your_source_tracker = tracker
   ```

3. **Add Clearing Function**:
   ```python
   if st.button("Clear Your Source Documentation", key="clear_your_source"):
       with st.spinner("Clearing Your Source documentation..."):
           from archon.crawl_your_source_docs import clear_existing_records
           result = clear_existing_records()
           st.success("Your Source documentation cleared successfully!")
   ```

### 4. Agent Integration

1. **Update archon_graph.py** to fetch documentation from both sources:
   ```python
   async def define_scope_with_reasoner(state: AgentState):
       # Get docs from all sources
       pydantic_ai_pages = await list_documentation_pages_helper(supabase, "pydantic_ai_docs")
       crawl4ai_pages = await list_documentation_pages_helper(supabase, "crawl4ai_docs")
       your_source_pages = await list_documentation_pages_helper(supabase, "your_source_docs")
       
       # Format documentation pages with source labels
       documentation_pages_str = "PYDANTIC AI DOCUMENTATION:\n" + "\n".join(pydantic_ai_pages)
       if crawl4ai_pages:
           documentation_pages_str += "\n\nCRAWL4AI DOCUMENTATION:\n" + "\n".join(crawl4ai_pages)
       if your_source_pages:
           documentation_pages_str += "\n\nYOUR SOURCE DOCUMENTATION:\n" + "\n".join(your_source_pages)
   ```

2. **Update pydantic_ai_coder.py** tools to support filtering by the new source:
   ```python
   @pydantic_ai_coder.tool
   async def retrieve_relevant_documentation(ctx: RunContext[PydanticAIDeps], user_query: str, source: Optional[str] = None) -> str:
       """
       Retrieve relevant documentation chunks based on the query with RAG.
       
       Args:
           ctx: The context including the Supabase client and OpenAI client
           user_query: The user's question or query
           source: Optional source filter - can be "pydantic_ai_docs", "crawl4ai_docs", or "your_source_docs". If not specified, searches all sources.
       """
       # Implementation logic
   ```

## Important Notes

1. **Reuse Patterns**: Always follow existing patterns when extending functionality.
2. **Error Handling**: Implement comprehensive error handling with detailed logs.
3. **Testing**: Test each component separately before integrating.
4. **Background Processing**: Use threaded or async operations for crawling to keep the UI responsive.
5. **Database Consistency**: Ensure source identifiers are consistent across all components.

## Example: Crawl4AI Implementation

For Crawl4AI, I,
1. Created `crawl_craw4ai_docs.py` based on `crawl_pydantic_ai_docs.py`
2. Modified it to retrieve URLs from the Crawl4AI sitemap
3. Updated metadata to use "crawl4ai_docs" as the source identifier
4. Added clearing functions specific to Crawl4AI content
5. Enhanced error handling and logging
6. Updated the UI to include Crawl4AI crawling options
7. Modified the agent to retrieve and use Crawl4AI documentation alongside Pydantic AI

This modular approach allows adding any number of additional documentation sources following the same pattern. 